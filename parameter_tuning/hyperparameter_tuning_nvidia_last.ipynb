{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac5af2-3057-4a98-9dcf-cd069e84415d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2159ec-c606-4585-859b-a60cc9458eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\stock_price_prediction\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout, MultiHeadAttention, LayerNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, MultiHeadAttention, LayerNormalization\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1116f3d-6f0f-4e6a-a292-395eb2877172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data (e.g., exponential decay)\n",
    "def simulate_ode(k=0.1, y0=1.0, t_max=10, num_points=1000):\n",
    "    t = np.linspace(0, t_max, num_points)\n",
    "    y = y0 * np.exp(-k * t)\n",
    "    return t, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0b7581-3fbf-465d-9f37-129ceb8b0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_stock_data(csv_path, date_column='Date', close_column='Close', test_size=0.2, val_size=0.1, time_steps=60):\n",
    "    \"\"\"\n",
    "    Prepares stock market price data for time series modeling.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing the data.\n",
    "        date_column (str): Name of the date column in the CSV.\n",
    "        close_column (str): Name of the closing price column in the CSV.\n",
    "        test_size (float): Proportion of the data for testing.\n",
    "        val_size (float): Proportion of the training data for validation.\n",
    "        time_steps (int): Number of past time steps to use for each sample.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, y_train: Training data and labels.\n",
    "        X_val, y_val: Validation data and labels.\n",
    "        X_test, y_test: Testing data and labels.\n",
    "        scaler: Fitted MinMaxScaler instance for inverse scaling.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(csv_path, parse_dates=[date_column])\n",
    "    data.sort_values(by=date_column, inplace=True)\n",
    "    \n",
    "    # Extract the 'close' column for scaling\n",
    "    close_prices = data[close_column].values.reshape(-1, 1)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_close = scaler.fit_transform(close_prices)\n",
    "    \n",
    "    # Create sequences of time_steps\n",
    "    X, y = [], []\n",
    "    for i in range(time_steps, len(scaled_close)):\n",
    "        X.append(scaled_close[i-time_steps:i])\n",
    "        y.append(scaled_close[i])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split data into train, validation, and test sets\n",
    "    train_size = int((1 - test_size) * len(X))\n",
    "    val_size = int(val_size * train_size)\n",
    "    \n",
    "    X_train, X_temp = X[:train_size], X[train_size:]\n",
    "    y_train, y_temp = y[:train_size], y[train_size:]\n",
    "    \n",
    "    X_val, X_test = X_temp[:val_size], X_temp[val_size:]\n",
    "    y_val, y_test = y_temp[:val_size], y_temp[val_size:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c1f9f4-3cc9-4dc2-a799-eedcd3d4e38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (528, 60, 1), (528, 1)\n",
      "Validation shape: (52, 60, 1), (52, 1)\n",
      "Test shape: (81, 60, 1), (81, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the directory of the current script\n",
    "current_dir = os.getcwd()\n",
    "# Construct the path to the data folder (same level as the tuning folder)\n",
    "data_folder = os.path.join(current_dir, \"..\", \"inputs\")\n",
    "data_file_path_nvidia = os.path.join(data_folder, \"nvidia_stock_cleaned.csv\")\n",
    "\n",
    "\n",
    "#data_path_google = \"D:/stock_price_prediction/inputs/nvidia_stock_cleaned.csv\"\n",
    "#data_path_google = \"./inputs/nvidia_stock_cleaned.csv\"\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, scaler = preprocess_stock_data(\n",
    "    data_file_path_nvidia, date_column='Date', close_column='Close', time_steps=60\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e234d7-b087-416a-b0ad-2874c2d74198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def objective(trial, model_type):\n",
    "    # Common hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "\n",
    "    # Define the model based on the type\n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'rnn':\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        units = trial.suggest_int('units', 32, 256, step=32)\n",
    "        for _ in range(num_layers):\n",
    "            model.add(SimpleRNN(units, activation='tanh', return_sequences=True if _ < num_layers - 1 else False))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    elif model_type == 'lstm':\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        units = trial.suggest_int('units', 32, 256, step=32)\n",
    "        for _ in range(num_layers):\n",
    "            model.add(LSTM(units, activation='tanh', return_sequences=True if _ < num_layers - 1 else False))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "    elif model_type == 'transformer':\n",
    "        num_heads = trial.suggest_int('num_heads', 2, 8)\n",
    "        key_dim = trial.suggest_int('key_dim', 16, 64, step=16)\n",
    "        ff_units = trial.suggest_int('ff_units', 32, 128, step=32)\n",
    "    \n",
    "        # Input Layer\n",
    "        input_layer = tf.keras.layers.Input(shape=(None, 1))\n",
    "    \n",
    "        # Transformer Encoder Layer\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(input_layer, input_layer)\n",
    "        attention_output = LayerNormalization()(attention_output)\n",
    "        attention_output = tf.keras.layers.Dense(ff_units, activation='relu')(attention_output)\n",
    "    \n",
    "         # Add a Dense layer for regression output\n",
    "        output_layer = tf.keras.layers.Dense(1)(attention_output)\n",
    "    \n",
    "         # Create a Model instead of Sequential\n",
    "        model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    elif model_type == 'neural_net':\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        units = trial.suggest_int('units', 32, 256, step=32)\n",
    "        for _ in range(num_layers):\n",
    "            model.add(Dense(units, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    elif model_type == 'ode':\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        units = trial.suggest_int('units', 32, 256, step=32)\n",
    "        activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "\n",
    "       # Input layer to handle sequences\n",
    "        input_layer = tf.keras.layers.Input(shape=(None, 1))\n",
    "    \n",
    "        # Stack Dense layers for feature extraction\n",
    "        x = input_layer\n",
    "        for _ in range(num_layers):\n",
    "            x = tf.keras.layers.Dense(units, activation=activation)(x)\n",
    "    \n",
    "        # Approximate the derivative using a Dense layer\n",
    "        derivative_layer = tf.keras.layers.Dense(units, activation=activation)(x)\n",
    "    \n",
    "        # Output layer for regression\n",
    "        output_layer = tf.keras.layers.Dense(1)(derivative_layer)\n",
    "    \n",
    "        # Define the functional model\n",
    "        model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Return the validation loss for Optuna to minimize\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcabb7-9914-4e5e-9e5b-f8d5f1fc0f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dab6c-46bc-4441-b1bb-b6dfb6779d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51f4ba-83ea-452e-8a24-e4f6c70ffde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_rnn = optuna.create_study(direction='minimize')\n",
    "study_rnn.optimize(lambda trial: objective(trial, model_type='rnn'), n_trials=100)\n",
    "print(\"Best RNN parameters:\", study_rnn.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f268f-4a2d-478f-9e29-e3dbcc1edd67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3e42b9c-c300-46f5-b779-fd7d788ffc50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c414e9-1095-4806-b643-b343b0c5f87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b98fe6-68d0-434a-ac11-0f38df1107e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd1561-051c-4092-bddd-096485f27162",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_lstm = optuna.create_study(direction='minimize')\n",
    "study_lstm.optimize(lambda trial: objective(trial, model_type='lstm'), n_trials=100)\n",
    "print(\"Best LSTM parameters:\", study_lstm.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6d6e8-c03b-4903-ac43-69d586758228",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b267d94-91bd-4c34-8a82-a5aee65c7c88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63724a-4988-4b36-8ab7-996a158ebc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_transformer = optuna.create_study(direction='minimize')\n",
    "study_transformer.optimize(lambda trial: objective(trial, model_type='transformer'), n_trials=100)\n",
    "print(\"Best Transformer parameters:\", study_transformer.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286e747-9eac-4703-9116-b5997baec16e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7148d71d-7e01-42a2-b9cf-6c2d7eb223da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1477768-718f-4901-9996-4db81c0aa892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3d1e9-c36e-424f-b9c8-b4203132863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_nn = optuna.create_study(direction='minimize')\n",
    "study_nn.optimize(lambda trial: objective(trial, model_type='neural_net'), n_trials=100)\n",
    "print(\"Best Neural Network parameters:\", study_nn.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327c03c-7061-4526-bcb4-8d78ba5f64b3",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82db049-b7e2-4258-af56-4ada45aee988",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08653131-a843-45dc-9216-5b488513f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_ode = optuna.create_study(direction='minimize')\n",
    "study_ode.optimize(lambda trial: objective(trial, model_type='ode'), n_trials=100)\n",
    "print(\"Best ODE parameters:\", study_ode.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1039aa8-00b0-495e-8e30-ed3faad8de91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "739bc4f8-8749-4eaf-96f1-b2e9d6bcfb08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb90065-176f-4877-967f-8375584e1865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00733cb8-0176-4d9d-b0da-0865603ffa10",
   "metadata": {},
   "source": [
    "Hereâ€™s how to use this function to print the architecture for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9d800-7e51-4f5f-bf97-9e5b6fad3e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53c0ac-a089-43ce-a309-9ab2c0df27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697fef1-2e35-44a0-90b7-15b79a286d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
