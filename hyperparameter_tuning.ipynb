{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac5af2-3057-4a98-9dcf-cd069e84415d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2159ec-c606-4585-859b-a60cc9458eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout, MultiHeadAttention, LayerNormalization\n",
    "\n",
    "# Generate synthetic data (e.g., exponential decay)\n",
    "def simulate_ode(k=0.1, y0=1.0, t_max=10, num_points=1000):\n",
    "    t = np.linspace(0, t_max, num_points)\n",
    "    y = y0 * np.exp(-k * t)\n",
    "    return t, y\n",
    "\n",
    "# Create synthetic data\n",
    "t, y = simulate_ode()\n",
    "X = t.reshape(-1, 1)  # Time as input\n",
    "y = y.reshape(-1, 1)  # State as output\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b7581-3fbf-465d-9f37-129ceb8b0fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e234d7-b087-416a-b0ad-2874c2d74198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial, model_type):\n",
    "    # Common hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "\n",
    "    # Define the model based on the type\n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'rnn':\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        units = trial.suggest_int('units', 32, 256, step=32)\n",
    "        for _ in range(num_layers):\n",
    "            model.add(SimpleRNN(units, activation='tanh', return_sequences=True if _ < num_layers - 1 else False))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    elif model_type == 'lstm':\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        units = trial.suggest_int('units', 32, 256, step=32)\n",
    "        for _ in range(num_layers):\n",
    "            model.add(LSTM(units, activation='tanh', return_sequences=True if _ < num_layers - 1 else False))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "    elif model_type == 'transformer':\n",
    "        num_heads = trial.suggest_int('num_heads', 2, 8)\n",
    "        key_dim = trial.suggest_int('key_dim', 16, 64, step=16)\n",
    "        model.add(MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, input_shape=(None, 1)))\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    elif model_type == 'neural_net':\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        units = trial.suggest_int('units', 32, 256, step=32)\n",
    "        for _ in range(num_layers):\n",
    "            model.add(Dense(units, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    elif model_type == 'ode':\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        units = trial.suggest_int('units', 32, 256, step=32)\n",
    "        model.add(Dense(units, activation='relu', input_dim=1))\n",
    "        for _ in range(num_layers):\n",
    "            model.add(Dense(units, activation='relu'))\n",
    "        model.add(Dense(1))  # Approximate derivative\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Return the validation loss for Optuna to minimize\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51f4ba-83ea-452e-8a24-e4f6c70ffde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_rnn = optuna.create_study(direction='minimize')\n",
    "study_rnn.optimize(lambda trial: objective(trial, model_type='rnn'), n_trials=20)\n",
    "print(\"Best RNN parameters:\", study_rnn.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd1561-051c-4092-bddd-096485f27162",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_lstm = optuna.create_study(direction='minimize')\n",
    "study_lstm.optimize(lambda trial: objective(trial, model_type='lstm'), n_trials=20)\n",
    "print(\"Best LSTM parameters:\", study_lstm.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63724a-4988-4b36-8ab7-996a158ebc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_transformer = optuna.create_study(direction='minimize')\n",
    "study_transformer.optimize(lambda trial: objective(trial, model_type='transformer'), n_trials=20)\n",
    "print(\"Best Transformer parameters:\", study_transformer.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3d1e9-c36e-424f-b9c8-b4203132863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_nn = optuna.create_study(direction='minimize')\n",
    "study_nn.optimize(lambda trial: objective(trial, model_type='neural_net'), n_trials=20)\n",
    "print(\"Best Neural Network parameters:\", study_nn.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08653131-a843-45dc-9216-5b488513f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_ode = optuna.create_study(direction='minimize')\n",
    "study_ode.optimize(lambda trial: objective(trial, model_type='ode'), n_trials=20)\n",
    "print(\"Best ODE parameters:\", study_ode.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37caa8-c67b-4e21-8e1e-78bedacab86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the best RNN model\n",
    "best_rnn_params = study_rnn.best_params\n",
    "print(\"Best RNN Parameters:\", best_rnn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb90065-176f-4877-967f-8375584e1865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00733cb8-0176-4d9d-b0da-0865603ffa10",
   "metadata": {},
   "source": [
    "Hereâ€™s how to use this function to print the architecture for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d718ef-a0b9-43d8-bfc3-ffee66a2d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, MultiHeadAttention, LayerNormalization\n",
    "\n",
    "def build_best_model(best_params, model_type):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'rnn':\n",
    "        for _ in range(best_params['num_layers']):\n",
    "            model.add(SimpleRNN(\n",
    "                units=best_params['units'],\n",
    "                activation='tanh',\n",
    "                return_sequences=True if _ < best_params['num_layers'] - 1 else False\n",
    "            ))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    elif model_type == 'lstm':\n",
    "        for _ in range(best_params['num_layers']):\n",
    "            model.add(LSTM(\n",
    "                units=best_params['units'],\n",
    "                activation='tanh',\n",
    "                return_sequences=True if _ < best_params['num_layers'] - 1 else False\n",
    "            ))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    elif model_type == 'transformer':\n",
    "        model.add(MultiHeadAttention(\n",
    "            num_heads=best_params['num_heads'],\n",
    "            key_dim=best_params['key_dim'],\n",
    "            input_shape=(None, 1)\n",
    "        ))\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    elif model_type == 'neural_net':\n",
    "        for _ in range(best_params['num_layers']):\n",
    "            model.add(Dense(\n",
    "                units=best_params['units'],\n",
    "                activation='relu'\n",
    "            ))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    elif model_type == 'ode':\n",
    "        model.add(Dense(\n",
    "            best_params['units'], activation='relu', input_dim=1\n",
    "        ))\n",
    "        for _ in range(best_params['num_layers']):\n",
    "            model.add(Dense(\n",
    "                best_params['units'], activation='relu'\n",
    "            ))\n",
    "        model.add(Dense(1))  # Approximate derivative\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9d800-7e51-4f5f-bf97-9e5b6fad3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Output Explanation\n",
    "Each summary() call will display:\n",
    "The layers of the model.\n",
    "The number of parameters in each layer.\n",
    "The total trainable and non-trainable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53c0ac-a089-43ce-a309-9ab2c0df27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Print the best RNN architecture\n",
    "best_rnn_model = build_best_model(study_rnn.best_params, 'rnn')\n",
    "print(\"Best RNN Architecture:\")\n",
    "best_rnn_model.summary()\n",
    "\n",
    "# Example: Print the best LSTM architecture\n",
    "best_lstm_model = build_best_model(study_lstm.best_params, 'lstm')\n",
    "print(\"\\nBest LSTM Architecture:\")\n",
    "best_lstm_model.summary()\n",
    "\n",
    "# Example: Print the best Transformer architecture\n",
    "best_transformer_model = build_best_model(study_transformer.best_params, 'transformer')\n",
    "print(\"\\nBest Transformer Architecture:\")\n",
    "best_transformer_model.summary()\n",
    "\n",
    "# Example: Print the best Neural Network architecture\n",
    "best_nn_model = build_best_model(study_nn.best_params, 'neural_net')\n",
    "print(\"\\nBest Neural Network Architecture:\")\n",
    "best_nn_model.summary()\n",
    "\n",
    "# Example: Print the best ODE architecture\n",
    "best_ode_model = build_best_model(study_ode.best_params, 'ode')\n",
    "print(\"\\nBest ODE Architecture:\")\n",
    "best_ode_model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
